apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kube-prometheus-stack
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: system
  destination:
    namespace: monitoring
    server: "https://kubernetes.default.svc"
  sources:
    - repoURL: "https://github.com/mhutter/rhea.git"
      targetRevision: HEAD
      path: apps/kube-prometheus-stack
    - repoURL: "https://prometheus-community.github.io/helm-charts"
      chart: kube-prometheus-stack
      targetRevision: "72.9.1"
      helm:
        # https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml
        # https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
        valuesObject:
          alertmanager:
            config:
              route:
                receiver: pagerduty
              receivers:
                - name: "null"
                - name: pagerduty
                  pagerduty_configs:
                    - service_key_file: /var/run/configs/pagerDutyServiceKey
                      send_resolved: true
                - name: telegram
                  telegram_configs:
                    - chat_id: 27184418
                      bot_token_file: /var/run/configs/telegramBotToken
            alertmanagerSpec:
              volumes:
                - name: configs
                  secret:
                    secretName: alertmanager-configs
              volumeMounts:
                - name: configs
                  mountPath: /var/run/configs

              resources:
                requests:
                  memory: 32Mi

          grafana:
            # Use admin credentials from existing secret
            admin:
              existingSecret: grafana-admin
            # Grafana Configuration
            envFromSecret: grafana-env
            grafana.ini:
              log:
                mode: console
              analytics:
                enabled: false
                reporting_enabled: false
                check_for_updates: false
                check_for_plugin_updates: false
              server:
                domain: grafana.mhnet.dev
                root_url: "https://grafana.mhnet.dev"
              auth.generic_oauth:
                enabled: true
                name: mhnet ID
                auth_url: "https://id.mhnet.app/authorize"
                token_url: "https://id.mhnet.app/api/oidc/token"
                api_url: "https://id.mhnet.app/api/oidc/userinfo"
                signout_redirect_url: "https://id.mhnet.app/api/oidc/end-session?post_logout_redirect_uri=https%3A%2F%2Fgrafana.mhnet.dev%2Flogin"
                scopes: openid profile email groups
                email_attribute_name: "email:primary"
                groups_attribute_path: "groups"
                role_attribute_path: "contains(groups[*], 'infra') && 'GrafanaAdmin' || 'Viewer'"
                use_pkce: true
                use_refresh_token: true

                allow_sign_up: true
                auto_login: true
                allow_assign_grafana_admin: true
            # Grafana Ingress
            ingress:
              enabled: true
              annotations:
                cert-manager.io/cluster-issuer: letsencrypt-production
              hosts: ["grafana.mhnet.dev"]
              tls:
                - hosts: ["grafana.mhnet.dev"]
                  secretName: grafana-ingress-tls
            # Grafana Persistence
            persistence:
              enabled: true
              type: sts
              accessModes: ["ReadWriteOnce"]
              size: 1Gi
            # Grafana Monitoring
            serviceMonitor:
              # Unset stupid scrapeTimeout (they default it to 30s, which is
              # incompatible with our scrapeInterval of 15s)
              scrapeTimeout: ""
            # Grafana Resources
            resources:
              requests:
                memory: 256Mi

          kubeEtcd:
            # no etcd on k3s
            enabled: false

          kubeControllerManager:
            endpoints: ["100.102.81.94"]
          kubeScheduler:
            endpoints: ["100.102.81.94"]

          kubeProxy:
            # kube-proxy is disabled (replaced by cilium)
            enabled: false

          prometheus-node-exporter:
            extraArgs:
              - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
              - --collector.filesystem.mount-points-exclude=^/(etc|run|var|sysroot/ostree/deploy|dev|proc|sys)($|/)
              - --collector.netdev.device-exclude=^(lxc.+|cilium_.+)$
              - --collector.systemd
              - --collector.systemd.unit-include=^(.*\.service)$
            extraHostVolumeMounts:
              - name: systemd
                hostPath: /var/run/dbus/system_bus_socket
                mountPath: /var/run/dbus/system_bus_socket
                readOnly: true

          prometheus:
            prometheusSpec:
              replicas: 1
              scrapeInterval: 15s
              retention: 30d

              # Another dumbfuckery from kube-prometheus-stack; if you don't
              # set selectors (e.g. to use ALL CONFIGS they set the helm release
              # label as the default selector.)
              podMonitorSelectorNilUsesHelmValues: false
              probeSelectorNilUsesHelmValues: false
              ruleSelectorNilUsesHelmValues: false
              scrapeConfigSelectorNilUsesHelmValues: false
              serviceMonitorSelectorNilUsesHelmValues: false

              resources:
                requests:
                  cpu: 200m
                  memory: 1Gi

              storageSpec:
                volumeClaimTemplate:
                  spec:
                    accessModes: ["ReadWriteOnce"]
                    resources:
                      requests:
                        storage: 40Gi

  ignoreDifferences:
    # Recent versions of K8s always return `apiVersion` and `kind` fields in
    # `volumeClaimTemplates`, but our templates don't have them so instead we
    # just tell ArgoCD to chill.
    - group: apps
      kind: StatefulSet
      name: kube-prometheus-stack-grafana
      jqPathExpressions:
        - ".spec.volumeClaimTemplates[].apiVersion"
        - ".spec.volumeClaimTemplates[].kind"

  syncPolicy:
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
    automated:
      prune: true
      selfHeal: true
